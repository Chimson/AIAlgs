TODO:

Make Example its own class
Make the DecisionTree class hold some data attributes, which will
  remove a lot of the function arguments in
Decide static vs non-static functions
Decide on public vs private members
Choose between get methods and Properties
Abstract the Condition Tree for conditions c(e) other than
  if the example has the condition/enum val then return true, otherwise
    return false
Example is Composition, maybe make it inherit a dictionary
Add a ToString() for Feature class
Feature class, for now, basically comes in 
	and helps retrieve the type string and Enum val values
	that Example uses in its Dictionary<string, Enum>
	Is this stupid?
Renove AddT and AddF functions in Node?
  maybe just make Node internal
Really study the example in the book, it tells you how find the 
  prediction for the target feature, and how the recursion works.
Condition Tree maybe works!
I think the book is saying, in the decision tree example, that select_split
  should return None, when all the values of the target feature agree, at
	the end of the recursion
Mine doesn't do this, which may have to do with my chosen sum_loss function
  But it does look like the target feature value has a clear majority
	I may need to adjust it, at the end of the recursion, when it splits on None
	   so that it returns a condition tree, where the root node is the majority val 
Double check that in Learner that it should create a deep copy of the conds example
  and pass it, with a removed cond, to the two recursive Learner calls
Needs a lot more testing with more training data and bigger decision trees
Example.ConvertToEnum() may parse a string to the wrong enum, if given 
  two seperate Enums types, that have the same value?
Each example has its own list of types for the Enums it contains in its
  dictionary, should there be like an Example's class that only
  holds one of theses lists?
There are some static functions in DecisionTree like find_mode(), sum_loss()
  that maybe should be moved to its own statistics/data science class
Add better sum_loss calculations
Make a version for a regression tree, when the target feature is 
  real valued
Allow for more complex conditions other than when the categorical 
  target values are equal (inside the internal, non leaf nodes)

Example of T(e) Tree function

Features: Sex->{Male, Female}, Gender->{White, Black}, Name->{Bob, Finn, Barb}
Cs = {Male, White}
Target Feature Y = Name
y = 1
Es = [
  Male, White, Finn
  Male, Black, Bob 
  Female, Black, Barb
]

DTL(Cs={Male, White}, Y=Name, Es):
  c = Male
  te = [Male, White, Finn
        Male, Black, Bob  
  ]
  t1 = DTL(Cs={White}, Y=Name, te):
    c = White
    te = [
      Male, White, Finn  
    ]
    t1 = DTL({}, Name, te)
      c = None
      v = Finn
      T([Male, White, Finn]) = Finn
      return T
    fe = [
      Male, Black, Bob
    ]
    t0 = DTL({}, Name, fe):
      c = None
      T([Male, Black, Bob]) = Bob
      return T
    T([Male, White, Finn]) = if White then Finn else None
    T([Male, Black, Bob]) = if White then None else Bob
    return T

Tree example
Features: Sex->{Male, Female}, Gender->{White, Black}, Name->{Bob, Finn, Barb}
Cs = {Male, White}
Target Feature Y = Name
Examples = {[Male, White, Finn],    (This is a complete set)
						[Male, Black, Bob],
            [Female, Black, Barb],
						[Female, White, Mags]}
				    
						Male
			1/	         \0
     White         White  
    1/   \0      1/   \0
  Finn   Bob    Mags   Barb

	

# -----

   Male   (t1)
  /    \
null   null

   White   (t2)
  /     \
null    null

       Finn
      /    \
     Male   White
     n  n   n   n



============

KMEANS
* I have vectors for each floating point, and then a list to hold the 
  entire data set, should this be changed?